{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Faster_R-CNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1JVVR04xj_PS1NfumGFiYi7Pwd8PmRXyj","authorship_tag":"ABX9TyPwLEI/g1bgpecnQQdM4nOY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CpTkucQfO024"},"source":["# Faster R-CNN training\n","\n","Training utilizes pytorch-lighning and Python COCO API.\n","\n","A more detailed description for regular PyTorch training implementation can be found at https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n","\n","The above mentioned guide is used as reference for this implementation."]},{"cell_type":"markdown","metadata":{"id":"I9xyoldy0kna"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"kDJHavHKic0e"},"source":["!pip install pytorch-lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uYL5_ijDfu97"},"source":["import time\n","import pytorch_lightning as pl\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import LambdaLR\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import os\n","import torch.utils.data\n","from torch.utils.data import random_split\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import CocoDetection\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from PIL import Image, ImageDraw\n","import matplotlib.pyplot as plt\n","from IPython.display import display\n","from pycocotools.coco import COCO\n","import gdown, zipfile \n","from contextlib import redirect_stdout\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ajYNPgb1yXp"},"source":["# Utility methods"]},{"cell_type":"code","metadata":{"id":"_sW0o-OtrQRK"},"source":["import json\n","import tempfile\n","\n","import numpy as np\n","import copy\n","import time\n","import torch\n","import torch._six\n","\n","from pycocotools.cocoeval import COCOeval\n","from pycocotools.coco import COCO\n","import pycocotools.mask as mask_util\n","\n","from collections import defaultdict\n","import torch.distributed as dist\n","\n","#import utils\n","####### UTILS #######\n","def is_dist_avail_and_initialized():\n","    if not dist.is_available():\n","        return False\n","    if not dist.is_initialized():\n","        return False\n","    return True\n","\n","\n","def get_world_size():\n","    if not is_dist_avail_and_initialized():\n","        return 1\n","    return dist.get_world_size()\n","\n","def all_gather(data):\n","    \"\"\"\n","    Run all_gather on arbitrary picklable data (not necessarily tensors)\n","    Args:\n","        data: any picklable object\n","    Returns:\n","        list[data]: list of data gathered from each rank\n","    \"\"\"\n","    world_size = get_world_size()\n","    if world_size == 1:\n","        return [data]\n","\n","    # serialized to a Tensor\n","    buffer = pickle.dumps(data)\n","    storage = torch.ByteStorage.from_buffer(buffer)\n","    tensor = torch.ByteTensor(storage).to(\"cuda\")\n","\n","    # obtain Tensor size of each rank\n","    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n","    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n","    dist.all_gather(size_list, local_size)\n","    size_list = [int(size.item()) for size in size_list]\n","    max_size = max(size_list)\n","\n","    # receiving Tensor from all ranks\n","    tensor_list = []\n","    for _ in size_list:\n","        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n","    if local_size != max_size:\n","        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n","        tensor = torch.cat((tensor, padding), dim=0)\n","    dist.all_gather(tensor_list, tensor)\n","\n","    data_list = []\n","    for size, tensor in zip(size_list, tensor_list):\n","        buffer = tensor.cpu().numpy().tobytes()[:size]\n","        data_list.append(pickle.loads(buffer))\n","\n","    return data_list\n","\n","def reduce_dict(input_dict, average=True):\n","    \"\"\"\n","    Args:\n","        input_dict (dict): all the values will be reduced\n","        average (bool): whether to do average or sum\n","    Reduce the values in the dictionary from all processes so that all processes\n","    have the averaged results. Returns a dict with the same fields as\n","    input_dict, after reduction.\n","    \"\"\"\n","    world_size = get_world_size()\n","    if world_size < 2:\n","        return input_dict\n","    with torch.no_grad():\n","        names = []\n","        values = []\n","        # sort the keys so that they are consistent across processes\n","        for k in sorted(input_dict.keys()):\n","            names.append(k)\n","            values.append(input_dict[k])\n","        values = torch.stack(values, dim=0)\n","        dist.all_reduce(values)\n","        if average:\n","            values /= world_size\n","        reduced_dict = {k: v for k, v in zip(names, values)}\n","    return reduced_dict\n","####### UTILS #######\n","\n","####### COCO_UTILS ######\n","def convert_to_coco_api(ds):\n","    coco_ds = COCO()\n","    ann_id = 0\n","    dataset = {'images': [], 'categories': [], 'annotations': []}\n","    categories = set()\n","    for img_idx in range(len(ds)):\n","        # find better way to get target\n","        # targets = ds.get_annotations(img_idx)\n","        img, targets = ds[img_idx]\n","        image_id = targets[\"image_id\"].item()\n","        img_dict = {}\n","        img_dict['id'] = image_id\n","        img_dict['height'] = img.shape[-2]\n","        img_dict['width'] = img.shape[-1]\n","        dataset['images'].append(img_dict)\n","        bboxes = targets[\"boxes\"]\n","        bboxes[:, 2:] -= bboxes[:, :2]\n","        bboxes = bboxes.tolist()\n","        labels = targets['labels'].tolist()\n","        areas = targets['area'].tolist()\n","        iscrowd = targets['iscrowd'].tolist()\n","        if 'masks' in targets:\n","            masks = targets['masks']\n","            # make masks Fortran contiguous for coco_mask\n","            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n","        if 'keypoints' in targets:\n","            keypoints = targets['keypoints']\n","            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n","        num_objs = len(bboxes)\n","        for i in range(num_objs):\n","            ann = {}\n","            ann['image_id'] = image_id\n","            ann['bbox'] = bboxes[i]\n","            ann['category_id'] = labels[i]\n","            categories.add(labels[i])\n","            ann['area'] = areas[i]\n","            ann['iscrowd'] = iscrowd[i]\n","            ann['id'] = ann_id\n","            if 'masks' in targets:\n","                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n","            if 'keypoints' in targets:\n","                ann['keypoints'] = keypoints[i]\n","                ann['num_keypoints'] = sum(k != 0 for k in keypoints[i][2::3])\n","            dataset['annotations'].append(ann)\n","            ann_id += 1\n","    dataset['categories'] = [{'id': i} for i in sorted(categories)]\n","    coco_ds.dataset = dataset\n","    coco_ds.createIndex()\n","    return coco_ds\n","\n","\n","def get_coco_api_from_dataset(dataset):\n","    for i in range(10):\n","        if isinstance(dataset, torchvision.datasets.CocoDetection):\n","            break\n","        if isinstance(dataset, torch.utils.data.Subset):\n","            dataset = dataset.dataset\n","    if isinstance(dataset, torchvision.datasets.CocoDetection):\n","        return dataset.coco\n","    return convert_to_coco_api(dataset)\n","####### COCO_UTILS ######\n","\n","####### COCO_EVAL ######\n","\n","class CocoEvaluator(object):\n","    def __init__(self, coco_gt, iou_types):\n","        assert isinstance(iou_types, (list, tuple))\n","        coco_gt = copy.deepcopy(coco_gt)\n","        self.coco_gt = coco_gt\n","\n","        self.iou_types = iou_types\n","        self.coco_eval = {}\n","        for iou_type in iou_types:\n","            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n","\n","        self.img_ids = []\n","        self.eval_imgs = {k: [] for k in iou_types}\n","\n","    def update(self, predictions):\n","        img_ids = list(np.unique(list(predictions.keys())))\n","        self.img_ids.extend(img_ids)\n","\n","        for iou_type in self.iou_types:\n","            results = self.prepare(predictions, iou_type)\n","            coco_dt = loadRes(self.coco_gt, results) if results else COCO()\n","            coco_eval = self.coco_eval[iou_type]\n","\n","            coco_eval.cocoDt = coco_dt\n","            coco_eval.params.imgIds = list(img_ids)\n","            img_ids, eval_imgs = evaluate(coco_eval)\n","\n","            self.eval_imgs[iou_type].append(eval_imgs)\n","\n","    def synchronize_between_processes(self):\n","        for iou_type in self.iou_types:\n","            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n","            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n","\n","    def accumulate(self):\n","        for coco_eval in self.coco_eval.values():\n","            coco_eval.accumulate()\n","\n","    def summarize(self):\n","        for iou_type, coco_eval in self.coco_eval.items():\n","            print(\"IoU metric: {}\".format(iou_type))\n","            coco_eval.summarize()\n","\n","    def prepare(self, predictions, iou_type):\n","        if iou_type == \"bbox\":\n","            return self.prepare_for_coco_detection(predictions)\n","        elif iou_type == \"segm\":\n","            return self.prepare_for_coco_segmentation(predictions)\n","        elif iou_type == \"keypoints\":\n","            return self.prepare_for_coco_keypoint(predictions)\n","        else:\n","            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n","\n","    def prepare_for_coco_detection(self, predictions):\n","        coco_results = []\n","        for original_id, prediction in predictions.items():\n","            if len(prediction) == 0:\n","                continue\n","\n","            boxes = prediction[\"boxes\"]\n","            boxes = convert_to_xywh(boxes).tolist()\n","            scores = prediction[\"scores\"].tolist()\n","            labels = prediction[\"labels\"].tolist()\n","\n","            coco_results.extend(\n","                [\n","                    {\n","                        \"image_id\": original_id,\n","                        \"category_id\": labels[k],\n","                        \"bbox\": box,\n","                        \"score\": scores[k],\n","                    }\n","                    for k, box in enumerate(boxes)\n","                ]\n","            )\n","        return coco_results\n","\n","    def prepare_for_coco_segmentation(self, predictions):\n","        coco_results = []\n","        for original_id, prediction in predictions.items():\n","            if len(prediction) == 0:\n","                continue\n","\n","            scores = prediction[\"scores\"]\n","            labels = prediction[\"labels\"]\n","            masks = prediction[\"masks\"]\n","\n","            masks = masks > 0.5\n","\n","            scores = prediction[\"scores\"].tolist()\n","            labels = prediction[\"labels\"].tolist()\n","\n","            rles = [\n","                mask_util.encode(np.array(mask[0, :, :, np.newaxis], order=\"F\"))[0]\n","                for mask in masks\n","            ]\n","            for rle in rles:\n","                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n","\n","            coco_results.extend(\n","                [\n","                    {\n","                        \"image_id\": original_id,\n","                        \"category_id\": labels[k],\n","                        \"segmentation\": rle,\n","                        \"score\": scores[k],\n","                    }\n","                    for k, rle in enumerate(rles)\n","                ]\n","            )\n","        return coco_results\n","\n","    def prepare_for_coco_keypoint(self, predictions):\n","        coco_results = []\n","        for original_id, prediction in predictions.items():\n","            if len(prediction) == 0:\n","                continue\n","\n","            boxes = prediction[\"boxes\"]\n","            boxes = convert_to_xywh(boxes).tolist()\n","            scores = prediction[\"scores\"].tolist()\n","            labels = prediction[\"labels\"].tolist()\n","            keypoints = prediction[\"keypoints\"]\n","            keypoints = keypoints.flatten(start_dim=1).tolist()\n","\n","            coco_results.extend(\n","                [\n","                    {\n","                        \"image_id\": original_id,\n","                        \"category_id\": labels[k],\n","                        'keypoints': keypoint,\n","                        \"score\": scores[k],\n","                    }\n","                    for k, keypoint in enumerate(keypoints)\n","                ]\n","            )\n","        return coco_results\n","\n","\n","def convert_to_xywh(boxes):\n","    xmin, ymin, xmax, ymax = boxes.unbind(1)\n","    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n","\n","\n","def merge(img_ids, eval_imgs):\n","    all_img_ids = all_gather(img_ids)\n","    all_eval_imgs = all_gather(eval_imgs)\n","\n","    merged_img_ids = []\n","    for p in all_img_ids:\n","        merged_img_ids.extend(p)\n","\n","    merged_eval_imgs = []\n","    for p in all_eval_imgs:\n","        merged_eval_imgs.append(p)\n","\n","    merged_img_ids = np.array(merged_img_ids)\n","    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n","\n","    # keep only unique (and in sorted order) images\n","    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n","    merged_eval_imgs = merged_eval_imgs[..., idx]\n","\n","    return merged_img_ids, merged_eval_imgs\n","\n","\n","def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n","    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n","    img_ids = list(img_ids)\n","    eval_imgs = list(eval_imgs.flatten())\n","\n","    coco_eval.evalImgs = eval_imgs\n","    coco_eval.params.imgIds = img_ids\n","    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n","\n","\n","#################################################################\n","# From pycocotools, just removed the prints\n","#################################################################\n","\n","def createIndex(self):\n","    anns, cats, imgs = {}, {}, {}\n","    imgToAnns, catToImgs = defaultdict(list), defaultdict(list)\n","    if 'annotations' in self.dataset:\n","        for ann in self.dataset['annotations']:\n","            imgToAnns[ann['image_id']].append(ann)\n","            anns[ann['id']] = ann\n","\n","    if 'images' in self.dataset:\n","        for img in self.dataset['images']:\n","            imgs[img['id']] = img\n","\n","    if 'categories' in self.dataset:\n","        for cat in self.dataset['categories']:\n","            cats[cat['id']] = cat\n","\n","    if 'annotations' in self.dataset and 'categories' in self.dataset:\n","        for ann in self.dataset['annotations']:\n","            catToImgs[ann['category_id']].append(ann['image_id'])\n","\n","    # create class members\n","    self.anns = anns\n","    self.imgToAnns = imgToAnns\n","    self.catToImgs = catToImgs\n","    self.imgs = imgs\n","    self.cats = cats\n","\n","\n","maskUtils = mask_util\n","\n","\n","def loadRes(self, resFile):\n","    \"\"\"\n","    Load result file and return a result api object.\n","    :param   resFile (str)     : file name of result file\n","    :return: res (obj)         : result api object\n","    \"\"\"\n","    res = COCO()\n","    res.dataset['images'] = [img for img in self.dataset['images']]\n","\n","    if isinstance(resFile, torch._six.string_classes):\n","        anns = json.load(open(resFile))\n","    elif type(resFile) == np.ndarray:\n","        anns = self.loadNumpyAnnotations(resFile)\n","    else:\n","        anns = resFile\n","    assert type(anns) == list, 'results in not an array of objects'\n","    annsImgIds = [ann['image_id'] for ann in anns]\n","    assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n","        'Results do not correspond to current coco set'\n","    if 'caption' in anns[0]:\n","        imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])\n","        res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]\n","        for id, ann in enumerate(anns):\n","            ann['id'] = id + 1\n","    elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n","        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n","        for id, ann in enumerate(anns):\n","            bb = ann['bbox']\n","            x1, x2, y1, y2 = [bb[0], bb[0] + bb[2], bb[1], bb[1] + bb[3]]\n","            if 'segmentation' not in ann:\n","                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n","            ann['area'] = bb[2] * bb[3]\n","            ann['id'] = id + 1\n","            ann['iscrowd'] = 0\n","    elif 'segmentation' in anns[0]:\n","        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n","        for id, ann in enumerate(anns):\n","            ann['area'] = maskUtils.area(ann['segmentation'])\n","            if 'bbox' not in ann:\n","                ann['bbox'] = maskUtils.toBbox(ann['segmentation'])\n","            ann['id'] = id + 1\n","            ann['iscrowd'] = 0\n","    elif 'keypoints' in anns[0]:\n","        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n","        for id, ann in enumerate(anns):\n","            s = ann['keypoints']\n","            x = s[0::3]\n","            y = s[1::3]\n","            x0, x1, y0, y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n","            ann['area'] = (x1 - x0) * (y1 - y0)\n","            ann['id'] = id + 1\n","            ann['bbox'] = [x0, y0, x1 - x0, y1 - y0]\n","\n","    res.dataset['annotations'] = anns\n","    createIndex(res)\n","    return res\n","\n","\n","def evaluate(self):\n","    '''\n","    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n","    :return: None\n","    '''\n","    p = self.params\n","    if p.useSegm is not None:\n","        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n","        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n","    p.imgIds = list(np.unique(p.imgIds))\n","    if p.useCats:\n","        p.catIds = list(np.unique(p.catIds))\n","    p.maxDets = sorted(p.maxDets)\n","    self.params = p\n","\n","    self._prepare()\n","    catIds = p.catIds if p.useCats else [-1]\n","\n","    if p.iouType == 'segm' or p.iouType == 'bbox':\n","        computeIoU = self.computeIoU\n","    elif p.iouType == 'keypoints':\n","        computeIoU = self.computeOks\n","    self.ious = {\n","        (imgId, catId): computeIoU(imgId, catId)\n","        for imgId in p.imgIds\n","        for catId in catIds}\n","\n","    evaluateImg = self.evaluateImg\n","    maxDet = p.maxDets[-1]\n","    evalImgs = [\n","        evaluateImg(imgId, catId, areaRng, maxDet)\n","        for catId in catIds\n","        for areaRng in p.areaRng\n","        for imgId in p.imgIds\n","    ]\n","    # this is NOT in the pycocotools code, but could be done outside\n","    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n","    self._paramsEval = copy.deepcopy(self.params)\n","    return p.imgIds, evalImgs\n","\n","#################################################################\n","# end of straight copy from pycocotools\n","#################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GcSmtGno0ysx"},"source":["# Set parameters"]},{"cell_type":"code","metadata":{"id":"quMmUu0-cWfN"},"source":["### Colab compatibility ###\n","import sys\n","sys.argv=['']\n","del sys\n","### Colab compatibility ###\n","\n","import argparse\n","\n","parser = argparse.ArgumentParser(description='VCC Arguments',add_help=False)\n","parser.add_argument('--num_workers', type=int, default=2, help=\"Number of data loader workers\")\n","parser.add_argument('--lr', type=float, default=0.005, help=\"Learning rate\")\n","parser.add_argument('--momentum', type=float, default=0.9, help=\"Optimizer momentum\")\n","parser.add_argument('--weight_decay', type=float, default=0.0005, help=\"Optimizer weight decay\")\n","parser.add_argument('--lr_schedule_step', type=int, default=3, help=\"Learning rate scheduler step size\")\n","parser.add_argument('--lr_schedule_gamma', type=float, default=0.1, help=\"Learning rate scheduler gamma value\")\n","parser.add_argument('--lr_warmup', type=int, default=1, help=\"Learning rate warmup\")\n","parser.add_argument('--batch_size', type=int, default=2, help=\"Batch Size of Train loaders\")\n","parser.add_argument('--num_epochs', type=int, default=30, help=\"Training Epochs\")\n","parser.add_argument('--eval_epoch', type=int, default=1, help=\"Validation frequency\")\n","parser.add_argument('--log_path', type=str, default='log/', help=\"Logging directory\")\n","parser.add_argument('--model_path', type=str, default='models/', help=\"Checkpoint directory\")\n","parser.add_argument('--model', type=str, default='fastrcnn', help=\"[fastrcnn|yolov5|yolov4]\")\n","parser.add_argument('--dataset', type=str, default='rvk', help=\"\")\n","parser.add_argument('--dataset_dir', type=str, default='data/', help=\"Dataset directory\")\n","parser.add_argument('--annotation_dir', type=str, default='annotation/', help=\"Annotation directory\")\n","parser.add_argument('--num_classes', type=int, default=2, help=\"Number of object detection classes\")\n","parser.add_argument('--val_split', type=float, default=.3, help=\"Validation split\")\n","\n","\n","args = parser.parse_args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"forkzCmbiF-2"},"source":["class DatasetCOCO(torch.utils.data.Dataset):\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # Own coco file\n","        coco = self.coco\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        # path for input image\n","        path = coco.loadImgs(img_id)[0][\"file_name\"]\n","        # open the input image\n","        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","        # get object categories ids\n","        cat_ids = coco.getCatIds()\n","        # get object categories names\n","        cats = coco.loadCats(coco.getCatIds())\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i][\"bbox\"][0]\n","            ymin = coco_annotation[i][\"bbox\"][1]\n","            xmax = xmin + coco_annotation[i][\"bbox\"][2]\n","            ymax = ymin + coco_annotation[i][\"bbox\"][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        boxes = boxes.reshape(-1, 4)\n","\n","        labels = []\n","        for i in range(num_objs):\n","          label = int(coco_annotation[i]['category_id'])\n","          labels.append(label)\n","\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i][\"area\"])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","        # Iscrowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Annotation is in dictionary format\n","        my_annotation = {}\n","        my_annotation[\"boxes\"] = boxes\n","        my_annotation[\"labels\"] = labels\n","        my_annotation[\"image_id\"] = img_id\n","        my_annotation[\"area\"] = areas\n","        my_annotation[\"iscrowd\"] = iscrowd\n","        my_annotation[\"categories\"] = cats\n","\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","        return img, my_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","    def get_labels(self):\n","        return list(map(lambda l: l['name'], self.coco.cats.values()))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BJZAtsdK03aB"},"source":["# Initialize Dataset"]},{"cell_type":"code","metadata":{"id":"KJrrBTlhiVVs"},"source":["class FCCDataModule(pl.LightningDataModule):\n","    def __init__(self, args):\n","        super().__init__()\n","        self.args = args\n","        self.data_dir = args.dataset_dir\n","        self.train_transform = transforms.Compose([transforms.ToTensor()])\n","        self.val_transform = transforms.Compose([transforms.ToTensor()])\n","\n","    def prepare_data(self):\n","      print(\"Dataset download\")\n","\n","      if (args.dataset == \"rvk\"):\n","        # download train images\n","        !curl -L \"https://app.roboflow.com/ds/lmjqJlEhnK?key=C5kBjzDuqk\" > roboflow.zip; \n","        os.system(\"unzip roboflow.zip -d {}\".format(args.dataset_dir)) \n","        self.dir = \"data/train\"\n","        !mkdir data/annotations/\n","        !mv data/train/_annotations.coco.json data/annotations/annotation.json\n","        self.annotation = \"data/annotations/annotation.json\"\n","\n","      else:\n","        raise ValueError('No valid dataset')\n","\n","    def setup(self, stage=None):\n","      print(\"Dataset setup\"\n","      data_set = DatasetCOCO(root=self.dir,annotation=self.annotation,transforms=self.train_transform)\n","      train_split_idx = int(len(data_set)*(1-args.val_split))\n","      val_split_idx = len(data_set)-train_split_idx\n","      self.coco_train, self.coco_val = random_split(data_set,[train_split_idx,val_split_idx])\n","\n","    def collate_fn(self, batch):\n","      if args.model == \"fastrcnn\": \n","        return tuple(zip(*batch))\n","      elif args.model == \"yolov5\":\n","        return list(zip(*batch))[0], list(zip(*batch))[1]\n","\n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.coco_train,\n","            batch_size=args.batch_size,\n","            shuffle=True,\n","            num_workers=args.num_workers,\n","            collate_fn=self.collate_fn,\n","        )\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.coco_val,\n","            batch_size=args.batch_size,\n","            shuffle=False,\n","            num_workers=args.num_workers,\n","            collate_fn=self.collate_fn,\n","        )\n","\n","# Init DataModule\n","dm = FCCDataModule(args)\n","dm.prepare_data()\n","dm.setup()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ai4T8jk708z6"},"source":["# Define and configure model"]},{"cell_type":"code","metadata":{"id":"OeaTXD9cff7p"},"source":["model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, args.num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJqnhUsSrABE"},"source":["class FCC(pl.LightningModule):\n","    def __init__(self, dataset, args):\n","        super().__init__()\n","        self.save_hyperparameters(args)\n","        \n","        self.model = model\n","\n","        self.coco = get_coco_api_from_dataset(dataset)\n","        self.iou_types = ['bbox'] #coco_utils: _get_iou_types(model)\n","\n","    def forward(self,imgs,annotations):\n","        # Overloaded fw pass\n","        if self.training: # For train - returns loss dict\n","          return self.model(imgs,annotations)\n","        else: # For test - return predictions and processed annotations for val metric calculations\n","          return self.model(imgs), annotations\n","\n","    def configure_optimizers(self):\n","        params = [p for p in model.parameters() if p.requires_grad]\n","        optimizer = torch.optim.SGD(params, lr=self.hparams.lr,\n","                            momentum=self.hparams.momentum, weight_decay=self.hparams.weight_decay)\n","        \n","        lf = lambda x: ((1 + math.cos(x * math.pi / self.hparams.num_epochs)) / 2) * (1 - 0.15) + 0.15  # cosine\n","        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n","\n","        return [optimizer], [scheduler]\n","\n","    def training_step(self,batch,batchidx):\n","        # Get batch\n","        imgs, annotations = batch\n","        # Get model train loss\n","        loss_dict = self(imgs, annotations)\n","        losses = sum(loss for loss in loss_dict.values())\n","        # reduce losses over all GPUs in case of multiple gpus\n","        loss_dict_reduced = reduce_dict(loss_dict)\n","        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n","        # Log metrics\n","        for key in loss_dict_reduced:\n","          self.log(key, loss_dict_reduced[key], prog_bar=True,on_step=True,on_epoch=True)\n","        self.log(\"lr\", self.optimizers(use_pl_optimizer=False).param_groups[0]['lr'], prog_bar=True)\n","        return {'loss': losses}\n","\n","    def on_validation_epoch_start(self):\n","        # Init new evaluator for each val epoch\n","        self.coco_evaluator = CocoEvaluator(self.coco, self.iou_types)\n","\n","    def validation_step(self,batch,batchidx):\n","        # Get batch\n","        imgs, annotations = batch\n","        # Predict\n","        pred_time = time.time()\n","        outputs, targets = self(imgs, annotations)\n","        # Log\n","        self.log('pred_time_sec/img', (time.time()-pred_time)/len(batch), prog_bar=True)\n","\n","        outputs = [{k: v.to(\"cpu\") for k, v in t.items()} for t in outputs] \n","        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n","        self.coco_evaluator.update(res)\n","\n","    def validation_epoch_end(self, training_step_outputs):  \n","        # Accumulate and log all metrics at the end of the val epoch\n","        with redirect_stdout(None): # workaround to mute std output\n","          self.coco_evaluator.synchronize_between_processes()\n","          self.coco_evaluator.accumulate()\n","          self.coco_evaluator.summarize() \n","        \n","        self.log('Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[0], prog_bar=True)\n","        self.log('Average Precision  (AP) @[ IoU=0.5       | area=   all | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[1], prog_bar=True)\n","        self.log('Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[2], prog_bar=True)\n","        self.log('Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[3], prog_bar=False)\n","        self.log('Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[4], prog_bar=False)\n","        self.log('Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[5], prog_bar=False)\n","        self.log('Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ]', self.coco_evaluator.coco_eval['bbox'].stats[6], prog_bar=False)\n","        self.log('Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ]', self.coco_evaluator.coco_eval['bbox'].stats[7], prog_bar=False)\n","        self.log('Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[8], prog_bar=False)\n","        self.log('Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[9], prog_bar=False)\n","        self.log('Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[10], prog_bar=False)\n","        self.log('Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ]', self.coco_evaluator.coco_eval['bbox'].stats[11], prog_bar=False)\n","\n","#pl.seed_everything(42, workers=True)\n","model = FCC(dm.train_dataloader().dataset, args)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Who_LXNN1F3X"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"lKSsK4eXgjby"},"source":["# Model checkpoint\n","checkpoint_callback = ModelCheckpoint(\n","    monitor='Average Precision  (AP) @[ IoU=0.5       | area=   all | maxDets=100 ]',\n","    dirpath=args.model_path,\n","    filename='fasterrcnn-{epoch:02d}-{AP_IoU_50:.2f}',\n","    #save_top_k=3,\n","    mode='max',\n",")\n","#early_stopping_callback = EarlyStopping(monitor=\"AP_IoU_50\")\n","# Init trainer\n","trainer = pl.Trainer(#min_epochs=args.num_epochs,\n","                     max_epochs=args.num_epochs,\n","                     gpus=[0],\n","                     #tpu_cores=8,\n","                     deterministic=True,    \n","                     check_val_every_n_epoch=args.eval_epoch,\n","                     logger=True,\n","                     progress_bar_refresh_rate=5,\n","                     callbacks=[checkpoint_callback],\n","                     auto_lr_find=True\n","                     #fast_dev_run = True\n","                     #auto_lr_find = \"hparams.lr\"\n","                     )\n","\n","\n","# Fit model\n","trainer.fit(model, datamodule=dm)"],"execution_count":null,"outputs":[]}]}