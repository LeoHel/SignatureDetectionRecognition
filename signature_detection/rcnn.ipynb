{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rcnn.ipynb","provenance":[],"mount_file_id":"1gbfydBcULX-Gt1r0Uh6ztraPyhkWRF-e","authorship_tag":"ABX9TyMpTbtzmr892Eo1kd2x0HLu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yRUwcRb62pau"},"source":["# Inference for Faster-RCNN"]},{"cell_type":"markdown","metadata":{"id":"Ww8QAXA_dWgw"},"source":["# Setup\n","\n","Enable GPU acceleration in Google Colab!"]},{"cell_type":"code","metadata":{"id":"kllgvPgqdRbJ"},"source":["import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","from google.colab.patches import cv2_imshow\n","\n","clear_output()\n","\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rjo8QaTVdbE4"},"source":["## run once to load model"]},{"cell_type":"code","metadata":{"id":"WUx0kFZDddcg"},"source":["import torch\n","import torchvision\n","import cv2\n","from torchvision import transforms\n","\n","device = 'cuda'\n","imgsz = 416\n","label_path = label_path='path-to-label-file' # path to file containing class label mapping according to VOC standard\n","\n","state_dict=torch.load('path-to-weight-file')\n","\n","# following is state_dict manipulation, as PyTorch Lighning (used for training) does save model state different from regular PyTorch\n","# prepending 'model.' is removed to load correct weights\n","\n","for key in list(state_dict.keys()):\n","  state_dict[key.replace('model.', '')] = state_dict.pop(key)\n","\n","# load model as usual for PyTorch\n","\n","model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False, num_classes=2)\n","model.load_state_dict(state_dict)\n","model.to(device)\n","\n","class_names = [name.strip() for name in open(label_path).readlines()]\n","\n","transform = transforms.Compose([\n"," transforms.ToTensor()\n","])\n","\n","img = cv2.imread('path-to-image') # image for initial run\n","img_r = cv2.resize(img, (416,416))\n","\n","scaley = img.shape[0]/imgsz\n","scalex = img.shape[1]/imgsz\n","\n","img_p = transforms.ToPILImage()(img_r)\n","img_t = transform(img_p)\n","batch_t = torch.unsqueeze(img_t, 0).to(device)\n","\n","model.eval()\n","\n","out = model(batch_t)\n","\n","print(out)\n","print(out[0]['boxes'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRt8tMA4fINN","executionInfo":{"status":"ok","timestamp":1633075139463,"user_tz":-120,"elapsed":175,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["def detect(path):\n","    img = cv2.imread(path)\n","    img_r = cv2.resize(img, (416,416))\n","\n","    scaley = img.shape[0]/imgsz\n","    scalex = img.shape[1]/imgsz\n","\n","    img_p = transforms.ToPILImage()(img_r)\n","    img_t = transform(img_p)\n","    batch_t = torch.unsqueeze(img_t, 0).to(device)\n","\n","    out = model(batch_t)\n","\n","    return out, img_r"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GGtyxi5edeDB"},"source":["# Inference on folder\n"]},{"cell_type":"code","metadata":{"id":"fDkhTKSndfFi"},"source":["import os\n","\n","directory = 'path-to-inference-folder'\n","\n","for filename in os.listdir(directory):\n","    if filename.endswith('.jpg') or filename.endswith('.png'):\n","\n","        path = os.path.join(directory, filename)\n","\n","        out, img = detect(path)\n","\n","        for i in range(len(out[0]['boxes'])):\n","            xyxy = out[0]['boxes'][i].data\n","            cv2.rectangle(img, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (255, 255, 255), 4)\n","            label = f'{class_names[out[0]['labels'][i]]}: {out[0]['scores'][i]:.2f}'\n","            cv2.putText(img, label,\n","                        (xyxy[0] - 10, xyxy[1] - 10),\n","                        cv2.FONT_HERSHEY_SIMPLEX,\n","                        1,  # font scale\n","                        (255, 255, 255),\n","                        2)  # line type\n","\n","        cv2_imshow(img)"],"execution_count":null,"outputs":[]}]}