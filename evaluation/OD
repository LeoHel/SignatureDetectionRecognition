{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OD","provenance":[],"collapsed_sections":["J4TWHT9eVygh","DFw3Zct-V-gL","AKlRnhBRahho","GxHoaFw_WGU_"],"mount_file_id":"1-lcuwk5rmbVoK5P10Z1JLtxO9KmtNwTW","authorship_tag":"ABX9TyNbEqWM7lEBWDP0StlvI61w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kgyECA4f16WI"},"source":["# Evaluation for OD\n","\n","Every implemented solution for OD is listed here. This notebook is structured for easier navigation.\n","\n","Results are saved in xywh format, the first two values representing x and y coordinates of the detection center and the last two values representing width and height prediction."]},{"cell_type":"markdown","metadata":{"id":"J4TWHT9eVygh"},"source":["# YOLOv5\n"]},{"cell_type":"markdown","metadata":{"id":"2LUVdgdbEzye"},"source":["## Setup\n","\n","Run each cell only once"]},{"cell_type":"code","metadata":{"id":"SRnB7lMOi5JM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633078318115,"user_tz":-120,"elapsed":10492,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}},"outputId":"e11f0741-3cbe-4be2-973d-d082d99868be"},"source":["!git clone https://github.com/roboflow-ai/yolov5\n","%cd yolov5\n","!pip install -qr requirements.txt\n","import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","\n","clear_output()\n","\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.9.0+cu102 _CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n"]}]},{"cell_type":"code","metadata":{"id":"_iiFQrKIkZEj"},"source":["import argparse\n","import time\n","from pathlib import Path\n","from google.colab.patches import cv2_imshow\n","\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","import numpy as np\n","import matplotlib\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \\\n","    strip_optimizer, set_logging, increment_path\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized\n","\n","# variables\n","path = 'path-to-image' # path to image for initial run\n","weights = 'path-to-weight-file' # path to weight file\n","imgsz = 416 # image size\n","conf_thres = 0.7 # confidence\n","iou_thres = 0.25 # iou\n","classes = '' # filter by class: 0 or 0 2 3\n","agnostic_nms = '' # class-agnostic NMS\n","device = select_device('0') # 'cpu' or '0'\n","half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n","    # Resize image to a 32-pixel-multiple rectangle\n","    shape = img.shape[:2]  # current shape [height, width]\n","    if isinstance(new_shape, int):\n","        new_shape = (new_shape, new_shape)\n","\n","    # Scale ratio (new / old)\n","    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n","    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n","        r = min(r, 1.0)\n","\n","    # Compute padding\n","    ratio = r, r  # width, height ratios\n","    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n","    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n","    if auto:  # minimum rectangle\n","        dw, dh = np.mod(dw, 32), np.mod(dh, 32)  # wh padding\n","    elif scaleFill:  # stretch\n","        dw, dh = 0.0, 0.0\n","        new_unpad = (new_shape[1], new_shape[0])\n","        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n","\n","    dw /= 2  # divide padding into 2 sides\n","    dh /= 2\n","\n","    if shape[::-1] != new_unpad:  # resize\n","        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n","    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n","    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n","    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n","    return img, ratio, (dw, dh)\n","\n","#load model - weights = path to weight file\n","model = attempt_load(weights, map_location=device)\n","imgsz = check_img_size(imgsz, s=model.stride.max())\n","if half:\n","  model.half()  # to FP16\n","\n","# Get names and colors\n","names = model.module.names if hasattr(model, 'module') else model.names\n","colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","\n","# Run inference\n","img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n","_ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n","\n","img1 = cv2.imread(path)\n","img0 = cv2.resize(img1, (imgsz, imgsz))\n","img = letterbox(img0, new_shape=imgsz)[0]\n","img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","img = np.ascontiguousarray(img)\n","\n","# scaling factors for x and y respective to original input size and chosen imgsz for prediction\n","scaley = img1.shape[0]/imgsz\n","scalex = img1.shape[1]/imgsz\n","\n","img = torch.from_numpy(img).to(device)\n","img = img.half() if half else img.float()  # uint8 to fp16/32\n","img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","if img.ndimension() == 3:\n","  img = img.unsqueeze(0)\n","\n","# Inference\n","pred = model(img, '')[0]\n","\n","# Apply NMS\n","pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic=agnostic_nms)\n","\n","# draw boxes\n","for *xyxy, conf, cls in reversed(pred[0]):\n","  label = '%s %.2f' % (names[int(cls)], conf)\n","  plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n","  \n","cv2_imshow(img0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-1sZUBU5BW-y"},"source":["## Optional: Validation set download"]},{"cell_type":"code","metadata":{"id":"OUZKjm2Ht9TM","executionInfo":{"status":"ok","timestamp":1633078330263,"user_tz":-120,"elapsed":2036,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["!curl -L 'https://app.roboflow.com/ds/WkDXLGYheD?key=dA9Sz4sFdJ' > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n","!mkdir /content/dets/\n","!mkdir /content/dets/text/\n","clear_output()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NmFvEnlNF8oU"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"lUfdYPawtmeS","executionInfo":{"status":"ok","timestamp":1633078473047,"user_tz":-120,"elapsed":5483,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["import os\n","import logging\n","directory = '/content/yolov5/valid/images' # image path if validation set is loaded\n","# directory = 'path-to-timing-set' # path to larger set for timing\n","label_dir = '/content/yolov5/valid/labels' # label path if validation set is loaded\n","det_path = '/content/dets/text/' # path where predictions are saved\n","\n","logging.basicConfig(filename='yolov5.log', level=logging.INFO, filemode='w')\n","logging.info('Started')\n","logging.info('Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0)))\n","\n","begin_time = time.time()\n","for filename in os.listdir(directory):\n","    if filename.endswith('.jpg') or filename.endswith('.png'):\n","\n","        t0 = time.time()\n","        path = os.path.join(directory, filename)\n","        img1 = cv2.imread(path)\n","        img0 = cv2.resize(img1, (imgsz, imgsz))\n","        img = letterbox(img0, new_shape=imgsz)[0]\n","        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","        img = np.ascontiguousarray(img)\n","\n","        # scaling factors for x and y respective to original input size and chosen imgsz for prediction\n","        scaley = img1.shape[0]/imgsz\n","        scalex = img1.shape[1]/imgsz\n","\n","        img = torch.from_numpy(img).to(device)\n","        img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        if img.ndimension() == 3:\n","          img = img.unsqueeze(0)\n","        \n","        t1=time.time()\n","        logging.info('time for image loading: %s' % (t1-t0))\n","\n","        t0 = time.time()\n","\n","        # Inference\n","        pred = model(img, '')[0]\n","\n","        # Apply NMS\n","        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic=agnostic_nms)\n","\n","        t1 = time.time()\n","\n","        logging.info('time for inference: %s' % (t1-t0))\n","          \n","        # location logging for mAP\n","        lines = []\n","        for *xyxy, conf, cls in reversed(pred[0]):\n","          label = '%s %.2f' % (names[int(cls)], conf)\n","          xyxy1 = [abs(xyxy[0] * scalex), abs(xyxy[1] * scaley), abs(xyxy[2] * scalex), abs(xyxy[3] * scaley)]\n","\n","          gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n","          xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","          line = (cls, conf, *xywh)  # label format\n","          lines.append(line)\n","          #with open(txt_path + '.txt', 'a') as f:\n","          #  f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n","\n","        # for the used validation tool, no '.' can appear in the file name\n","        # every '.' is therefore removed\n","        # this also has to be done for every image manually (or automated)\n","        # a mapping between predictions and images in the evaluation tool is not possible if names do not match\n","        # ground truth annotation names also need to match prediction and image names\n","\n","        filename = os.path.splitext(filename)[0]\n","        filename = filename.replace('.', '')\n","        for line in lines:\n","          with open(det_path + filename + '.txt', 'a') as f:\n","            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n","\n","end_time = time.time()\n","logging.info('overall time: %s' % (end_time-begin_time))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zhAYoe_gE6Bz"},"source":["## Parameter count"]},{"cell_type":"code","metadata":{"id":"P5S2Gi49QnlF"},"source":["pp=0\n","for p in list(model.parameters()):\n","    nn=1\n","    for s in list(p.size()):\n","        nn = nn*s\n","    pp += nn\n","print(pp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFw3Zct-V-gL"},"source":["# YOLOv4"]},{"cell_type":"markdown","metadata":{"id":"zbQELp90IIdB"},"source":["## Setup\n","\n","Run each cell only once"]},{"cell_type":"code","metadata":{"id":"D2iS5hwUWAqJ"},"source":["!git clone https://github.com/WongKinYiu/PyTorch_YOLOv4.git\n","%cd /content/PyTorch_YOLOv4\n","!pip uninstall -y pyyaml\n","!pip install pyyaml\n","!pip install git+https://github.com/JunnYu/mish-cuda.git\n","\n","import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","from google.colab.patches import cv2_imshow\n","\n","clear_output()\n","\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dbj5jIn8WFzZ"},"source":["import argparse\n","import os\n","import platform\n","import shutil\n","import time\n","from pathlib import Path\n","\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","\n","from utils.google_utils import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import (\n","    check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, strip_optimizer)\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized\n","\n","from models.models import *\n","from utils.datasets import *\n","from utils.general import *\n","\n","# variables\n","path = 'path-to-image' # path to image for initial run\n","weights = 'path-to-weight-file' # path to weight file\n","imgsz = 416 # image size\n","conf_thres = 0.2 # confidence\n","iou_thres = 0.25 # iou\n","classes = '' # filter by class: 0 or 0 2 3\n","agnostic_nms = '' # class-agnostic NMS\n","device = select_device('0') # 'cpu' or '0'\n","half = device.type != 'cpu'  # half precision only supported on CUDA\n","half = False\n","cfg = 'cfg/yolov4-tiny.cfg'\n","names = 'path-to-classes-file' # path to file containing class name mapping\n","\n","\n","def load_classes(path):\n","    # Loads *.names file at 'path'\n","    with open(path, 'r') as f:\n","        names = f.read().split('\\n')\n","    return list(filter(None, names))  # filter removes empty strings (such as last line)\n","\n","# helper?\n","def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n","    # Resize image to a 32-pixel-multiple rectangle\n","    shape = img.shape[:2]  # current shape [height, width]\n","    if isinstance(new_shape, int):\n","        new_shape = (new_shape, new_shape)\n","\n","    # Scale ratio (new / old)\n","    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n","    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n","        r = min(r, 1.0)\n","\n","    # Compute padding\n","    ratio = r, r  # width, height ratios\n","    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n","    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n","    if auto:  # minimum rectangle\n","        dw, dh = np.mod(dw, 32), np.mod(dh, 32)  # wh padding\n","    elif scaleFill:  # stretch\n","        dw, dh = 0.0, 0.0\n","        new_unpad = (new_shape[1], new_shape[0])\n","        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n","\n","    dw /= 2  # divide padding into 2 sides\n","    dh /= 2\n","\n","    if shape[::-1] != new_unpad:  # resize\n","        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n","    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n","    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n","    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n","    return img, ratio, (dw, dh)\n","\n","model = Darknet(cfg, imgsz).cuda()\n","try:\n","    model.load_state_dict(torch.load(weights, map_location=device)['model'])\n","except:\n","    load_darknet_weights(model, weights)\n","model.to(device).eval()\n","\n","if half:\n","    model.half()  # to FP16\n","\n","# Get names and colors\n","names = load_classes(names)\n","colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","\n","\n","# Run inference\n","img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n","_ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n","\n","img1 = cv2.imread(path)\n","img0 = cv2.resize(img1, (imgsz, imgsz))\n","img = letterbox(img0, new_shape=imgsz)[0]\n","img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","img = np.ascontiguousarray(img)\n","\n","# scaling factors for x and y respective to original input size and chosen imgsz for prediction\n","scaley = img1.shape[0]/imgsz\n","scalex = img1.shape[1]/imgsz\n","\n","img = torch.from_numpy(img).to(device)\n","img = img.half() if half else img.float()  # uint8 to fp16/32\n","img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","if img.ndimension() == 3:\n","  img = img.unsqueeze(0)\n","\n","# Inference\n","pred = model(img, '')[0]\n","\n","# Apply NMS\n","pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic=agnostic_nms)\n","\n","\n","\n","# draw boxes\n","if (pred[0]) != None:\n","  for *xyxy, conf, cls in reversed(pred[0]):\n","    label = '%s %.2f' % (names[int(cls)], conf)\n","    plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n","  \n","cv2_imshow(img0)\n","#cv2_imshow(img1)\n","\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dxyDE6f-GTO3"},"source":["## Optional: Validation set download"]},{"cell_type":"code","metadata":{"id":"uUzybOq1Xhl1","executionInfo":{"status":"ok","timestamp":1633079036591,"user_tz":-120,"elapsed":3113,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["!curl -L 'https://app.roboflow.com/ds/WkDXLGYheD?key=dA9Sz4sFdJ' > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n","!mkdir /content/dets/\n","!mkdir /content/dets/text/\n","clear_output()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzoD5cPwId7E"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"GmKSOUP0Y9eZ","executionInfo":{"status":"ok","timestamp":1633079041613,"user_tz":-120,"elapsed":5026,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["import os\n","import logging\n","directory = '/content/PyTorch_YOLOv4/valid/images' # image path if validation set is loaded\n","# directory = 'path-to-timing-set' # path to larger set for timing\n","label_dir = '/content/PyTorch_YOLOv4/valid/labels' # label path if validation set is loaded\n","det_path = '/content/dets/text/' # path where predictions are saved\n","\n","logging.basicConfig(filename='yolov4.log', level=logging.INFO, filemode='w')\n","logging.info('Started')\n","logging.info('Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0)))\n","\n","begin_time = time.time()\n","for filename in os.listdir(directory):\n","    if filename.endswith('.jpg') or filename.endswith('.png'):\n","        \n","        t0 = time.time()\n","        path = os.path.join(directory, filename)\n","        img1 = cv2.imread(path)\n","        img0 = cv2.resize(img1, (imgsz, imgsz))\n","        img = letterbox(img0, new_shape=imgsz)[0]\n","        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","        img = np.ascontiguousarray(img)\n","\n","        # scaling factors for x and y respective to original input size and chosen imgsz for prediction\n","        scaley = img1.shape[0]/imgsz\n","        scalex = img1.shape[1]/imgsz\n","\n","        img = torch.from_numpy(img).to(device)\n","        img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        if img.ndimension() == 3:\n","          img = img.unsqueeze(0)\n","\n","        t1=time.time()\n","        logging.info('time for image loading: %s' % (t1-t0))\n","\n","        t0 = time.time()\n","\n","        # Inference\n","        pred = model(img, '')[0]\n","\n","        # Apply NMS\n","        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic=agnostic_nms)\n","\n","        t1 = time.time()\n","\n","        logging.info('time for inference: %s' % (t1-t0))\n","\n","        # location logging for mAP\n","          \n","        lines = []\n","        if (pred[0]) != None:\n","          for *xyxy, conf, cls in reversed(pred[0]):\n","            label = '%s %.2f' % (names[int(cls)], conf)\n","            xyxy1 = [abs(xyxy[0] * scalex), abs(xyxy[1] * scaley), abs(xyxy[2] * scalex), abs(xyxy[3] * scaley)]\n","\n","            gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n","            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","            line = (cls, conf, *xywh)  # label format\n","            lines.append(line)\n","\n","        # for the used validation tool, no '.' can appear in the file name\n","        # every '.' is therefore removed\n","        # this also has to be done for every image manually (or automated)\n","        # a mapping between predictions and images in the evaluation tool is not possible if names do not match\n","        # ground truth annotation names also need to match prediction and image names\n","\n","          filename = os.path.splitext(filename)[0]\n","          filename = filename.replace('.', '')\n","          for line in lines:\n","            with open(det_path + filename + '.txt', 'a') as f:\n","              f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n","\n","end_time = time.time()\n","logging.info('overall time: %s' % (end_time-begin_time))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6tb-bFfBIVb9"},"source":["## Parameter count"]},{"cell_type":"code","metadata":{"id":"7YJX8sVfQTRy"},"source":["pp=0\n","for p in list(model.parameters()):\n","    nn=1\n","    for s in list(p.size()):\n","        nn = nn*s\n","    pp += nn\n","print(pp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKlRnhBRahho"},"source":["# YOLOv3\n"]},{"cell_type":"markdown","metadata":{"id":"qDYEMnixKBQj"},"source":["## Setup\n","\n","Run each cell only once"]},{"cell_type":"code","metadata":{"id":"u3sDugTuajlh"},"source":["!git clone https://github.com/ultralytics/yolov3\n","%cd yolov3\n","\n","import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","\n","clear_output()\n","\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kRVYNfxaarSm"},"source":["import argparse\n","import time\n","from pathlib import Path\n","from google.colab.patches import cv2_imshow\n","\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","import numpy as np\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n","    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box\n","from utils.plots import colors, plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized\n","\n","# variables\n","path = 'path-to-image' # path to image for initial run\n","weights = 'path-to-weight-file' # path to weight file\n","imgsz = 416 # image size\n","conf_thres = 0.7 # confidence\n","iou_thres = 0.25 # iou\n","classes = None # filter by class: 0 or 0 2 3\n","agnostic_nms = '' # class-agnostic NMS\n","device = select_device('0') # 'cpu' or '0'\n","half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n","    # Resize and pad image while meeting stride-multiple constraints\n","    shape = img.shape[:2]  # current shape [height, width]\n","    if isinstance(new_shape, int):\n","        new_shape = (new_shape, new_shape)\n","\n","    # Scale ratio (new / old)\n","    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n","    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n","        r = min(r, 1.0)\n","\n","    # Compute padding\n","    ratio = r, r  # width, height ratios\n","    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n","    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n","    if auto:  # minimum rectangle\n","        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n","    elif scaleFill:  # stretch\n","        dw, dh = 0.0, 0.0\n","        new_unpad = (new_shape[1], new_shape[0])\n","        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n","\n","    dw /= 2  # divide padding into 2 sides\n","    dh /= 2\n","\n","    if shape[::-1] != new_unpad:  # resize\n","        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n","    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n","    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n","    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n","    return img, ratio, (dw, dh)\n","\n","# Load model\n","model = attempt_load(weights, map_location=device)  # load FP32 model\n","stride = int(model.stride.max())  # model stride\n","imgsz = check_img_size(imgsz, s=stride)  # check img_size\n","names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n","colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","if half:\n","    model.half()  # to FP16\n","\n","# Run inference\n","if device.type != 'cpu':\n","    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n","t0 = time.time()\n","#for path, img, im0s, vid_cap in dataset:\n","img1 = cv2.imread(path)\n","img0 = cv2.resize(img1, (imgsz, imgsz))\n","img = letterbox(img0, new_shape=imgsz)[0]\n","img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","img = np.ascontiguousarray(img)\n","\n","# scaling factors for x and y respective to original input size and chosen imgsz for prediction\n","scaley = img1.shape[0]/imgsz\n","scalex = img1.shape[1]/imgsz\n","    \n","img = torch.from_numpy(img).to(device)\n","img = img.half() if half else img.float()  # uint8 to fp16/32\n","img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","if img.ndimension() == 3:\n","    img = img.unsqueeze(0)\n","\n","# Inference\n","t1 = time_synchronized()\n","pred = model(img, '')[0]\n","\n","# Apply NMS\n","pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic=agnostic_nms)\n","t2 = time_synchronized()\n","\n","# draw boxes\n","for *xyxy, conf, cls in reversed(pred[0]):\n","  label = '%s %.2f' % (names[int(cls)], conf)\n","  plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n","  \n","cv2_imshow(img0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59JrFKN7Hnmi"},"source":["## Optional: Validation set download"]},{"cell_type":"code","metadata":{"id":"gMy7bZayazmt","executionInfo":{"status":"ok","timestamp":1633079270727,"user_tz":-120,"elapsed":3210,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["!curl -L 'https://app.roboflow.com/ds/WkDXLGYheD?key=dA9Sz4sFdJ' > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n","!mkdir /content/dets/\n","!mkdir /content/dets/text/\n","clear_output()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IMA5yk1oKQqw"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"_OzFsQDSbFn4"},"source":["import os\n","import logging\n","# directory = 'path-to-timing-set' # path to larger set for timing\n","directory = '/content/yolov3/valid/images' # image path if validation set is loaded\n","label_dir = '/content/yolov3/valid/labels' # label path if validation set is loaded\n","det_path = '/content/dets/text/' # path where predictions are saved\n","\n","logging.basicConfig(filename='yolov3-tiny.log', level=logging.INFO, filemode='w')\n","logging.info('Started')\n","logging.info('Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0)))\n","\n","begin_time = time.time()\n","for filename in os.listdir(directory):\n","    if filename.endswith('.jpg') or filename.endswith('.png'):\n","        \n","        t0 = time.time()\n","        path = os.path.join(directory, filename)\n","        img1 = cv2.imread(path)\n","        img0 = cv2.resize(img1, (imgsz, imgsz))\n","        img = letterbox(img0, new_shape=imgsz)[0]\n","        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","        img = np.ascontiguousarray(img)\n","\n","        # scaling factors for x and y respective to original input size and chosen imgsz for prediction\n","        scaley = img1.shape[0]/imgsz\n","        scalex = img1.shape[1]/imgsz\n","\n","        img = torch.from_numpy(img).to(device)\n","        img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        if img.ndimension() == 3:\n","          img = img.unsqueeze(0)\n","\n","        t1 = time.time()\n","        logging.info('time for image loading: %s' % (t1-t0))\n","\n","        t0 = time.time()\n","        # Inference\n","        pred = model(img, '')[0]\n","\n","        # Apply NMS\n","        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic=agnostic_nms)\n","\n","        t1 = time.time()\n","\n","        logging.info('time for inference: %s' % (t1-t0))\n","\n","        # location logging for mAP\n","          \n","        #cv2_imshow(img0)\n","        lines = []\n","        for *xyxy, conf, cls in reversed(pred[0]):\n","          label = '%s %.2f' % (names[int(cls)], conf)\n","          xyxy1 = [abs(xyxy[0] * scalex), abs(xyxy[1] * scaley), abs(xyxy[2] * scalex), abs(xyxy[3] * scaley)]\n","\n","          gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n","          xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","          line = (cls, conf, *xywh)  # label format\n","          lines.append(line)\n","\n","        # for the used validation tool, no '.' can appear in the file name\n","        # every '.' is therefore removed\n","        # this also has to be done for every image manually (or automated)\n","        # a mapping between predictions and images in the evaluation tool is not possible if names do not match\n","        # ground truth annotation names also need to match prediction and image names\n","\n","        filename = os.path.splitext(filename)[0]\n","        filename = filename.replace('.', '')\n","        for line in lines:\n","          with open(det_path + filename + '.txt', 'a') as f:\n","            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n","\n","end_time = time.time()\n","logging.info('overall time: %s' % (end_time-begin_time))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcdscmojJ4CA"},"source":["## Parameter count"]},{"cell_type":"code","metadata":{"id":"oSHEE9H4SxVT"},"source":["pp=0\n","for p in list(model.parameters()):\n","    nn=1\n","    for s in list(p.size()):\n","        nn = nn*s\n","    pp += nn\n","print(pp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9MTQrWvScIIU"},"source":["# SSD\n"]},{"cell_type":"markdown","metadata":{"id":"N7MapGjhKs1r"},"source":["## Setup\n","\n","Run each cell only once"]},{"cell_type":"code","metadata":{"id":"wqJtgCIAcKPG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633079400585,"user_tz":-120,"elapsed":5719,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}},"outputId":"93e65051-85dc-44aa-f90e-1d21faf49af3"},"source":["!git clone https://github.com/qfgaohao/pytorch-ssd.git\n","%cd pytorch-ssd/\n","\n","import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","from google.colab.patches import cv2_imshow\n","\n","clear_output()\n","\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.9.0+cu102 _CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n"]}]},{"cell_type":"code","metadata":{"id":"CXYuwbKAcXr_"},"source":["from vision.ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor\n","from vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd, create_mobilenetv1_ssd_predictor\n","from vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite, create_mobilenetv1_ssd_lite_predictor\n","from vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite, create_squeezenet_ssd_lite_predictor\n","from vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite, create_mobilenetv2_ssd_lite_predictor\n","from vision.ssd.mobilenetv3_ssd_lite import create_mobilenetv3_large_ssd_lite, create_mobilenetv3_small_ssd_lite\n","from vision.utils.misc import Timer\n","import cv2\n","import sys\n","import time\n","\n","import torchvision.transforms as transforms\n","from numpy import random\n","from google.colab.patches import cv2_imshow\n","\n","label_path='path-to-lable-file' # path to file containing class label mapping according to VOC standard\n","\n","# choose model\n","# uncomment respective model_path and net_type\n","# model_path needs to contain path to weights file of respective model architecture\n","\n","# model_path='path-to-weight-file'\n","# net_type='mb1-ssd'\n","\n","# model_path='path-to-weight-file'\n","# net_type='mb1-ssd-lite'\n","\n","model_path='path-to-weight-file'\n","net_type='mb2-ssd-lite'\n","\n","# model_path='path-to-weight-file'\n","# net_type='mb3-large-ssd-lite'\n","\n","# model_path='path-to-weight-file'\n","# net_type='mb3-small-ssd-lite'\n","\n","# model_path='path-to-weight-file'\n","# net_type='sq-ssd-lite'\n","\n","class_names = [name.strip() for name in open(label_path).readlines()]\n","\n","if net_type == 'vgg16-ssd':\n","    net = create_vgg_ssd(len(class_names), is_test=True)\n","elif net_type == 'mb1-ssd':\n","    net = create_mobilenetv1_ssd(len(class_names), is_test=True)\n","elif net_type == 'mb1-ssd-lite':\n","    net = create_mobilenetv1_ssd_lite(len(class_names), is_test=True)\n","elif net_type == 'mb2-ssd-lite':\n","    net = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)\n","elif net_type == 'mb3-large-ssd-lite':\n","    net = create_mobilenetv3_large_ssd_lite(len(class_names), is_test=True)\n","elif net_type == 'mb3-small-ssd-lite':\n","    net = create_mobilenetv3_small_ssd_lite(len(class_names), is_test=True)\n","elif net_type == 'sq-ssd-lite':\n","    net = create_squeezenet_ssd_lite(len(class_names), is_test=True)\n","else:\n","    print('The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite.')\n","    sys.exit(1)\n","    \n","net.load(model_path)\n","\n","if net_type == 'vgg16-ssd':\n","    predictor = create_vgg_ssd_predictor(net, candidate_size=200, device=0)\n","elif net_type == 'mb1-ssd':\n","    predictor = create_mobilenetv1_ssd_predictor(net, candidate_size=200, device=0)\n","elif net_type == 'mb1-ssd-lite':\n","    predictor = create_mobilenetv1_ssd_lite_predictor(net, candidate_size=200, device=0)\n","elif net_type == 'mb2-ssd-lite' or net_type == 'mb3-large-ssd-lite' or net_type == 'mb3-small-ssd-lite':\n","    predictor = create_mobilenetv2_ssd_lite_predictor(net, candidate_size=200, device=0)\n","elif net_type == 'sq-ssd-lite':\n","    predictor = create_squeezenet_ssd_lite_predictor(net, candidate_size=200, device=0)\n","else:\n","    predictor = create_vgg_ssd_predictor(net, candidate_size=200, device=0)\n","\n","# variables\n","path = 'path-to-image' # path to image for initial inference\n","imgsz = 300 # image size\n","conf_thres = 0.4 # confidence\n","\n","colors = [[random.randint(0, 255) for _ in range(3)] for _ in class_names]\n","\n","img1 = cv2.imread(path)\n","img0 = cv2.resize(img1, (imgsz, imgsz))\n","\n","# scaling factors for x and y respective to original input size and chosen imgsz for prediction\n","scaley = img1.shape[0]/imgsz\n","scalex = img1.shape[1]/imgsz\n","\n","boxes, labels, probs = predictor.predict(img0, prob_threshold=conf_thres)\n","\n","print(boxes)\n","print(labels)\n","print(probs)\n","# draw boxes\n","for i in range(boxes.size(0)):\n","  if probs[i] > 0.1:\n","    box = boxes[i, :]\n","    cv2.rectangle(img0, (box[0], box[1]), (box[2], box[3]), (255, 255, 255), 4)\n","    label = f'{class_names[labels[i]]}: {probs[i]:.2f}'\n","    cv2.putText(img0, label,\n","                (box[0] - 10, box[1] - 10),\n","                cv2.FONT_HERSHEY_SIMPLEX,\n","                1,  # font scale\n","                (255, 255, 255),\n","                2)  # line type\n","  \n","cv2_imshow(img0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drKOU1Xfl5TP","executionInfo":{"status":"ok","timestamp":1633079750250,"user_tz":-120,"elapsed":228,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["def xyxy2xywh(x):\n","    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n","    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n","    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n","    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n","    y[:, 2] = x[:, 2] - x[:, 0]  # width\n","    y[:, 3] = x[:, 3] - x[:, 1]  # height\n","    return y"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ijYITKnkJCb7"},"source":["## Optional: Validation set download"]},{"cell_type":"code","metadata":{"id":"4MCW9wJ1eE5r","executionInfo":{"status":"ok","timestamp":1633079754274,"user_tz":-120,"elapsed":1679,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["!curl -L 'https://app.roboflow.com/ds/WkDXLGYheD?key=dA9Sz4sFdJ' > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n","!mkdir /content/dets/\n","!mkdir /content/dets/text/\n","clear_output()"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWH64PxBK-JL"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"C0qarK2Oc54c"},"source":["import os\n","import time\n","import logging\n","# directory = 'path-to-timing-set' # path to larger set for timing\n","directory = '/content/pytorch-ssd/valid/images' # image path if validation set is loaded\n","label_dir = '/content/pytorch-ssd/valid/labels' # label path if validation set is loaded\n","det_path = '/content/dets/text/' # path where predictions are saved\n","\n","logging.basicConfig(filename='ssd.log', level=logging.INFO, filemode='w')\n","logging.info('Started')\n","logging.info('Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0)))\n","\n","begin_time = time.time()\n","for filename in os.listdir(directory):\n","    if filename.endswith('.jpg') or filename.endswith('.png'):\n","        \n","        t0 = time.time()\n","        path = os.path.join(directory, filename)\n","        img1 = cv2.imread(path)\n","        img0 = cv2.resize(img1, (imgsz, imgsz))\n","\n","        t1 = time.time()\n","        logging.info('time for image loading: %s' % (t1-t0))\n","\n","        t0 = time.time()\n","        boxes, labels, probs = predictor.predict(cv2.resize(img0, (300, 300)), prob_threshold=conf_thres)\n","        t1 = time.time()\n","\n","        logging.info('time for inference: %s' % (t1-t0))\n","\n","        # location logging for mAP\n","\n","        lines = []\n","\n","        for i in range(boxes.size(0)):\n","          if probs[i] > 0.1:\n","            box = boxes[i, :]\n","            \n","            x = (box[0] + box[2]) / 2  # x center\n","            y = (box[1] + box[3]) / 2  # y center\n","            w = box[2] - box[0]  # width\n","            h = box[3] - box[1]  # height\n","\n","            gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n","\n","            xywh = (xyxy2xywh(torch.tensor(box).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","\n","            line = (labels[i], probs[i], *xywh)  # label format\n","            lines.append(line)\n","\n","        # for the used validation tool, no '.' can appear in the file name\n","        # every '.' is therefore removed\n","        # this also has to be done for every image manually (or automated)\n","        # a mapping between predictions and images in the evaluation tool is not possible if names do not match\n","        # ground truth annotation names also need to match prediction and image names\n","\n","        filename = os.path.splitext(filename)[0]\n","        filename = filename.replace('.', '')\n","        for line in lines:\n","          with open(det_path + filename + '.txt', 'a') as f:\n","            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n","\n","end_time = time.time()\n","logging.info('overall time: %s' % (end_time-begin_time))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fxh6KD9gK0Gt"},"source":["## Parameter count"]},{"cell_type":"code","metadata":{"id":"iIZ5jaEkRtFM"},"source":["pp=0\n","for p in list(net.parameters()):\n","    nn=1\n","    for s in list(p.size()):\n","        nn = nn*s\n","    pp += nn\n","print(pp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8MHiTNxrmf0"},"source":["# Faster R-CNN\n"]},{"cell_type":"markdown","metadata":{"id":"9hmx-V_YMI2i"},"source":["## Setup\n","\n","Run each cell only once"]},{"cell_type":"code","metadata":{"id":"qpRz6pF8rtXt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633080052412,"user_tz":-120,"elapsed":4500,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}},"outputId":"2363aa21-433f-4756-9776-8fd53e331269"},"source":["import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","from google.colab.patches import cv2_imshow\n","\n","clear_output()\n","\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.9.0+cu102 _CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n"]}]},{"cell_type":"code","metadata":{"id":"Hno3GP05rvfF"},"source":["import torch\n","import torchvision\n","\n","device = 'cuda'\n","\n","state_dict=torch.load('path-to-weight-file') # path to weight file\n","\n","# following is state_dict manipulation, as PyTorch Lighning (used for training) does save model state different from regular PyTorch\n","# prepending 'model.' is removed to load correct weights\n","\n","for key in list(state_dict.keys()):\n","  state_dict[key.replace('model.', '')] = state_dict.pop(key)\n","\n","# load model as usual for PyTorch\n","\n","model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False, num_classes=2)\n","model.load_state_dict(state_dict)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KI2MMy7zr9IQ","executionInfo":{"status":"ok","timestamp":1633080120671,"user_tz":-120,"elapsed":226,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["def xyxy2xywh(x):\n","    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n","    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n","    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n","    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n","    y[:, 2] = x[:, 2] - x[:, 0]  # width\n","    y[:, 3] = x[:, 3] - x[:, 1]  # height\n","    return y"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"99nXBFmTrxVU"},"source":["import cv2\n","from torchvision import transforms\n","\n","imgsz = 416\n","\n","transform = transforms.Compose([\n"," #transforms.Resize(416),\n"," transforms.ToTensor()\n","])\n","\n","img = cv2.imread('path-to-image') # path to image for initial inference\n","img_r = cv2.resize(img, (416,416))\n","\n","scaley = img.shape[0]/imgsz\n","scalex = img.shape[1]/imgsz\n","\n","img_p = transforms.ToPILImage()(img_r)\n","img_t = transform(img_p)\n","batch_t = torch.unsqueeze(img_t, 0).to(device)\n","\n","model.eval()\n","\n","out = model(batch_t)\n","\n","print(out)\n","print(out[0]['boxes'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlavVRItKhAz"},"source":["## Optional: Validation set download"]},{"cell_type":"code","metadata":{"id":"orrgYLsasFhy","executionInfo":{"status":"ok","timestamp":1633080178685,"user_tz":-120,"elapsed":1718,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["!curl -L 'https://app.roboflow.com/ds/WkDXLGYheD?key=dA9Sz4sFdJ' > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n","!mkdir /content/dets/\n","!mkdir /content/dets/text/\n","clear_output()"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3EkwV58MaVP"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"YRcHGisKr9y-"},"source":["import os\n","import time\n","import logging\n","# directory = 'path-to-timing-set' # path to larger set for timing\n","directory = '/content/valid/images' # image path if validation set is loaded\n","label_dir = '/content/valid/labels' # label path if validation set is loaded\n","det_path = '/content/dets/text/' # path where predictions are saved\n","\n","logging.basicConfig(filename='rcnn.log', level=logging.INFO, filemode='w')\n","logging.info('Started')\n","logging.info('Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0)))\n","\n","begin_time = time.time()\n","\n","for filename in os.listdir(directory):\n","    if filename.endswith('.jpg') or filename.endswith('.png'):\n","\n","        t0 = time.time()\n","        path = os.path.join(directory, filename)\n","        img = cv2.imread(path)\n","        img_r = cv2.resize(img, (416,416))\n","\n","        img_p = transforms.ToPILImage()(img_r)\n","        img_t = transform(img_p)\n","        batch_t = torch.unsqueeze(img_t, 0).to(device)\n","\n","        t1 = time.time()\n","        logging.info('time for image loading: %s' % (t1-t0))\n","\n","        t0 = time.time()\n","        out = model(batch_t)\n","        t1 = time.time()\n","\n","        logging.info('time for inference: %s' % (t1-t0))\n","\n","        # location logging for mAP\n","\n","        lines = []\n","        for i in range(out[0]['boxes'].size(0)):\n","          #if probs[i] > 0.1:\n","            box = out[0]['boxes'][i, :]\n","            x = (box[0] + box[2]) / 2  # x center\n","            y = (box[1] + box[3]) / 2  # y center\n","            w = box[2] - box[0]  # width\n","            h = box[3] - box[1]  # height\n","\n","            gn = torch.tensor(img_r.shape)[[1, 0, 1, 0]].to(device)\n","\n","            xywh = (xyxy2xywh(torch.tensor(box).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","\n","            line = (out[0]['labels'][i], out[0]['scores'][i], *xywh)  # label format\n","            lines.append(line)\n","\n","        # for the used validation tool, no '.' can appear in the file name\n","        # every '.' is therefore removed\n","        # this also has to be done for every image manually (or automated)\n","        # a mapping between predictions and images in the evaluation tool is not possible if names do not match\n","        # ground truth annotation names also need to match prediction and image names\n","\n","        filename = os.path.splitext(filename)[0]\n","        filename = filename.replace('.', '')\n","        for line in lines:\n","          with open(det_path + filename + '.txt', 'a') as f:\n","            f.write(('%g ' * len(line)).rstrip() % line + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P6AhxN80MOOS"},"source":["## Parameter count"]},{"cell_type":"code","metadata":{"id":"BduUE9RuTaM6"},"source":["pp=0\n","for p in list(model.parameters()):\n","    nn=1\n","    for s in list(p.size()):\n","        nn = nn*s\n","    pp += nn\n","print(pp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QSHbni1XY_Bj"},"source":["# Template matching"]},{"cell_type":"markdown","metadata":{"id":"Uw2InLTXOxB4"},"source":["## Setup\n","\n","Run each cell only once"]},{"cell_type":"code","metadata":{"id":"pfTv4AypZB3r"},"source":["import cv2\n","from IPython.display import Image, clear_output  # to display images\n","from google.colab.patches import cv2_imshow\n","\n","def detectAndDisplay(frame):\n","    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    frame_gray = cv2.equalizeHist(frame_gray)\n","    dets = cascade.detectMultiScale(frame_gray)\n","    for (x,y,w,h) in dets:\n","        frame = cv2.rectangle(frame, (x, y), (x+w, y+h), (255,0,255), 4 )\n","    cv2_imshow(frame)\n","\n","cascade_name = 'path-to-cascade.xml-file'\n","cascade = cv2.CascadeClassifier()\n","cascade.load(cascade_name)\n","\n","img = cv2.imread('path-to-image') # path to image for initial inference\n","detectAndDisplay(cv2.resize(img, (1000, 750)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQf150QjPKSf","executionInfo":{"status":"ok","timestamp":1633080491316,"user_tz":-120,"elapsed":391,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["def xyxy2xywh(x):\n","    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n","    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n","    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n","    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n","    y[:, 2] = x[:, 2] - x[:, 0]  # width\n","    y[:, 3] = x[:, 3] - x[:, 1]  # height\n","    return y"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m4J9HpCELLhO"},"source":["## Optional: Validation set download"]},{"cell_type":"code","metadata":{"id":"tH4b-KQkcBQV","executionInfo":{"status":"ok","timestamp":1633080480789,"user_tz":-120,"elapsed":3990,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["!curl -L 'https://app.roboflow.com/ds/WkDXLGYheD?key=dA9Sz4sFdJ' > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n","!mkdir /content/dets/\n","!mkdir /content/dets/text/\n","clear_output()"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cMeTZk84O355"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"MsHQI3fUcD9S","executionInfo":{"status":"ok","timestamp":1633080502495,"user_tz":-120,"elapsed":10020,"user":{"displayName":"Leon Heller","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16860286207446221136"}}},"source":["import os\n","import time\n","import logging\n","import torch\n","# directory = 'path-to-timing-set' # path to larger set for timing\n","directory = '/content/valid/images' # image path if validation set is loaded\n","label_dir = '/content/valid/labels' # label path if validation set is loaded\n","det_path = '/content/dets/text/' # path where predictions are saved\n","\n","logging.basicConfig(filename='template.log', level=logging.INFO, filemode='w')\n","logging.info('Started - 1/4th res')\n","\n","begin_time = time.time()\n","\n","for filename in os.listdir(directory):\n","    if filename.endswith('.jpg') or filename.endswith('.png'):\n","        \n","        t0 = time.time()\n","        path = os.path.join(directory, filename)\n","        img = cv2.imread(path)\n","        img = cv2.resize(img, (1000, 750))\n","        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","        img_gray = cv2.equalizeHist(img_gray)\n","\n","        t1 = time.time()\n","        logging.info('time for image loading: %s' % (t1-t0))\n","\n","        t0 = time.time()\n","        dets = cascade.detectMultiScale(img_gray)\n","        t1 = time.time()\n","\n","        logging.info('time for inference: %s' % (t1-t0))\n","\n","        # location logging for mAP\n","\n","        lines = []\n","        for det in dets:\n","          #if probs[i] > 0.1:\n","            box = [det[0], det[1], (det[0]+det[2]), (det[1]+det[3])]\n","\n","            gn = torch.tensor(img_gray.shape)[[1, 0, 1, 0]]\n","\n","            xywh = (xyxy2xywh(torch.tensor(box).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","\n","            line = (1, 1, *xywh)  # label format\n","            lines.append(line)\n","\n","        filename = os.path.splitext(filename)[0]\n","        filename = filename.replace('.', '')\n","        for line in lines:\n","          with open(det_path + filename + '.txt', 'a') as f:\n","            f.write(('%g ' * len(line)).rstrip() % line + '\\n')"],"execution_count":5,"outputs":[]}]}